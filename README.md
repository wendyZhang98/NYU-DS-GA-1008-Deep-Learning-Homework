# DS-GA-1008-DeepLearning

This repository contains my notes and homework from **DS-GA: Deep Learning**, a graduate-level course taught by **Joan Bruna**, **Yann LeCun**, and **Alfredo Canziani** at NYU.

The course focuses on the theoretical foundations and practical implementations of modern deep learning models, covering both supervised and unsupervised representation learning.

---

## üìö Course Overview

This course explores the latest advances in **deep learning and representation learning**, focusing on:
- **Supervised & Unsupervised Learning**
- **Embedding Methods & Metric Learning**
- **Convolutional & Recurrent Networks**
- **Energy-Based and Generative Models**
- **Optimization & Inference Techniques**

Applications include **computer vision**, **natural language understanding**, and **speech recognition**.

---

## üóìÔ∏è Lecture Topics & Highlights

| **Week** | **Topic** | **Highlights** |
|-----------|------------|----------------|
| **01** | General Introduction | History, motivation, neural network basics, and backpropagation. |
| **02** | Learning Paradigms: SL & SSL | Overview of supervised and self-supervised learning architectures. |
| **03** | Basic Statistical Learning | Understanding the curse of dimensionality and bias-variance tradeoff. |
| **04** | Learning in the Physical World | Geometric domains and data manifolds in learning. |
| **05** | Geometric Priors | Invariance, equivariance, and scale separation principles. |
| **06** | The GDL Blueprint | Graphs, grids, and group theory foundations for geometric deep learning. |
| **07** | GDL Applications | Practical implementations of graph-based neural models. |
| **08** | Energy-Based Models I | Introduction to energy-based modeling and inference. |
| **09** | Energy-Based Models II | Training techniques for energy-based models. |
| **10** | Energy-Based Models III | Target propagation and autoencoders. |
| **11** | Energy-Based Models IV | Variational inference, VAEs, and energy-based GANs. |
| **12** | Variational Inference & EBM | Connections between VAEs, GANs, and EBM training dynamics. |
| **13** | Graphs & Relational Learning | Understanding relational representations and structured prediction. |
| **14** | Control | Control theory and optimization in deep systems. |
| **15** | Open Problems & Closing Remarks | Discussions on open research questions and NeurIPS-level advances. |

---

## üí° What I Learned

From this course, I developed a deep understanding of the **mathematical foundations of modern deep learning** ‚Äî from optimization and representation learning to geometric priors and energy-based models.  
Key takeaways include:
- The role of **symmetry, invariance, and equivariance** in neural architectures.  
- How **self-supervised and unsupervised learning** bridge representation gaps.  
- How **energy-based models** unify inference and generation under a common framework.  
- Connections between **graph-based learning**, **variational inference**, and **control theory**.  
- Exposure to cutting-edge research directions from leaders in deep learning.

---

## ‚úçÔ∏è Author

**Wenxin Zhang**  
*NYU ‚Äî DS-GA: Deep Learning (Instructors: Joan Bruna, Yann LeCun, Alfredo Canziani)*
